{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d3c870",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Named Entity Recognition Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4bc80a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39bd3527-1830-412a-b08c-0f2e9a4d77c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modin.pandas as pd_modin\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f5308",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Load text dataset with entity tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38f2bff4-711d-409d-a451-3e71be5dec87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "\n",
    "l=Faker('en_GB') \n",
    "# f=open(\"test.csv\",\"r\")\n",
    "# k=csv.reader(f)\n",
    "\n",
    "with open(\"big.csv\",\"a\") as g:\n",
    "    w=csv.writer(g)\n",
    "    w.writerow(('id','name','address','college','company','dob','age'))\n",
    "    for i in range(1000000):\n",
    "\n",
    "        w.writerow((i+1,l.name(),l.address(),random.choice(['psg','sona','amirta','anna university']),random.choice(['CTS','INFY','HTC']),(random.randrange(1950,1995,1),random.randrange(1,13,1),random.randrange(1,32,1)),random.choice(range(0,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e260ae-9dfb-4f8c-b779-a3da2ed9f129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f3211-1c3a-4206-9576-974480340a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "pd.read_csv(\"large.csv\")\n",
    "print(time.perf_counter() -start)\n",
    "\n",
    "start = time.perf_counter()\n",
    "pd_modin.read_csv(\"large.csv\")\n",
    "print(time.perf_counter() -start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10d658-8c91-4d27-8e5d-4b99331c1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pd.read_csv(\"large.cs\")\n",
    "print(time.time() -start)\n",
    "\n",
    "start = time.time()\n",
    "pd_modin.read_csv(\"large.csv\")\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedf34e-90b5-4657-98ea-ef168e29f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pd.read_csv(\"large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a9cfc-d8bc-4d83-8112-254dbe1d1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pd_modin.read_csv(\"large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03064ebd-ec0a-4c20-ac8f-c098aec5ef63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ee2af-52a8-4200-9002-dc382a97bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3a3f8-74e1-4e38-b4f7-9c38ca17beb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61305be6-3f06-4c3a-938b-abe2fec0b6c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4190807342529297"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = pd.read_csv(\"ner.csv\")\n",
    "time.time()- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726c3eb4-3a22-470e-8582-c9f418b1e788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31632183901092503"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "dataset = pd.read_csv(\"ner.csv\")\n",
    "perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ca27ac-a2a4-4da2-96e9-5c8835459326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab4d21e0-6d5b-4b78-abd7-7b11d991721d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6143596172332764"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "modin_dataset = pd_modin.read_csv(\"ner.csv\")\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef5c259-46e1-4575-94ee-29957a262062",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"Sentence #\"] = dataset[\"Sentence #\"].fillna(method='ffill')\n",
    "sentences, targets = [], []\n",
    "for sent_i, x in dataset.groupby(\"Sentence #\"):\n",
    "    words = x[\"Word\"].tolist()\n",
    "    tags = x[\"Tag\"].tolist()\n",
    "    sentences.append(words)\n",
    "    targets.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04a0b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Number of sentences in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1fe25e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22862"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3373f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Text encoding\n",
    "\n",
    "Convert each word into subwords and their respective subword ids such that Bert can work with the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566b85f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "# tokenize words\n",
    "PRETRAINED = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED)\n",
    "sentences_encoded = tokenizer(\n",
    "    sentences, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True, max_length=150, add_special_tokens=False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b846e80d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22862, 150])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_encoded[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7319a81",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Target encoding\n",
    "\n",
    "Convert the NER tags into tensors such that Bert can work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b0639c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-tim': 0,\n",
       " 'B-tim': 1,\n",
       " 'B-org': 2,\n",
       " 'B-gpe': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-per': 5,\n",
       " 'I-eve': 6,\n",
       " 'B-art': 7,\n",
       " 'I-art': 8,\n",
       " 'I-gpe': 9,\n",
       " 'O': 10,\n",
       " 'I-org': 11,\n",
       " 'I-per': 12,\n",
       " 'B-eve': 13,\n",
       " 'B-nat': 14,\n",
       " 'B-geo': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from ner tag to number\n",
    "tag2idx = {tag: i for i, tag in enumerate(set(t for ts in targets for t in ts))}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e34bc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pad the target tensors because sentences have different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c30cc0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22862, 150])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = sentences_encoded[\"input_ids\"].shape[1]\n",
    "targets_encoded = torch.empty((0, max_len), dtype=torch.long)\n",
    "\n",
    "for sent_idx, target in enumerate(targets):\n",
    "    enc = torch.full(size=(max_len,), fill_value=tag2idx['O'], dtype=torch.long)\n",
    "    # repeat ner tag for each subword\n",
    "    for word_idx, tag in enumerate(target):\n",
    "        span = sentences_encoded.word_to_tokens(sent_idx, word_idx)\n",
    "        # ignore words that tokenizer did not understand e.g. special characters\n",
    "        if span is not None:\n",
    "            start, end = span\n",
    "            enc[start:end] = tag2idx[tag]\n",
    "    targets_encoded = torch.vstack((targets_encoded, enc))\n",
    "\n",
    "targets_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055220a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Show the first sample and its target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d815b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sentences_encoded[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f04998",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10, 10, 10, 10, 15, 10, 10, 10, 10, 10, 15, 10, 10, 10, 10, 10,\n",
       "          3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_encoded[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116fdbb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train/Test split\n",
    "\n",
    "Split dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c82a27",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train sentences: 18289', 'Test sentences: 4572')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * len(sentences_encoded.input_ids))\n",
    "test_size = int(0.2 * len(sentences_encoded.input_ids))\n",
    "train_sentences = sentences_encoded[:train_size]\n",
    "train_targets = targets_encoded[:train_size]\n",
    "test_sentences = sentences_encoded[train_size:train_size+test_size]\n",
    "test_targets = targets_encoded[train_size:train_size+test_size]\n",
    "(f\"Train sentences: {len(train_targets)}\", f\"Test sentences: {len(test_targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03259852",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = torch.tensor(self.sentences[index].ids)\n",
    "        mask = torch.tensor(self.sentences[index].attention_mask)\n",
    "        labels = self.labels[index].clone()\n",
    "\n",
    "        return {\n",
    "            'ids': ids,\n",
    "            'mask': mask,\n",
    "            'tags': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "training_set = NERDataset(train_sentences, train_targets)\n",
    "testing_set = NERDataset(test_sentences, test_targets)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "testing_loader = DataLoader(testing_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c15201",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0dfbb1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load a pretrained Bert model to fine-tune for multi-class classification of NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a4797e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = BertForTokenClassification.from_pretrained(PRETRAINED, num_labels=len(tag2idx))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9005f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "loss: 2.88024\n",
      "loss: 1.80421\n",
      "loss: 1.15632\n",
      "loss: 1.01469\n",
      "loss: 0.89944\n",
      "loss: 0.98938\n",
      "loss: 0.92672\n",
      "loss: 0.7426\n",
      "loss: 0.62529\n",
      "loss: 0.72075\n",
      "loss: 0.69993\n",
      "loss: 0.59267\n",
      "epoch: 2\n",
      "loss: 0.71214\n",
      "loss: 0.74181\n",
      "loss: 0.65504\n",
      "loss: 0.50718\n",
      "loss: 0.36026\n",
      "loss: 0.32124\n",
      "loss: 0.38675\n",
      "loss: 0.39118\n",
      "loss: 0.43116\n",
      "loss: 0.44601\n",
      "loss: 0.53579\n",
      "loss: 0.42692\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(1, 3):\n",
    "    print(\"epoch:\", epoch)\n",
    "    model.train()\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        data = {k: v.to(device) for k, v in data.items()}\n",
    "        output = model(data[\"ids\"], attention_mask=data[\"mask\"], labels=data[\"tags\"])\n",
    "        loss = output[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 100 == 0:\n",
    "            print(\"loss:\", round(loss.detach().cpu().item(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb3d25",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test model\n",
    "\n",
    "Compute classification metric of Bert model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72dd228e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_trues = [], []\n",
    "\n",
    "for data in testing_loader:\n",
    "    data = {k: v.to(device) for k, v in data.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(data[\"ids\"], attention_mask=data[\"mask\"], labels=data[\"tags\"])\n",
    "    loss = output[0]\n",
    "    logits = output[1].detach().cpu()\n",
    "    mask = data[\"mask\"].cpu()\n",
    "\n",
    "    label_ids = data[\"tags\"].cpu()\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    for i in range(pred_ids.shape[0]):\n",
    "        # remove pad predictions\n",
    "        pred_ids_non_pad = pred_ids[i, mask[i]]\n",
    "        label_ids_non_pad = label_ids[i, mask[i]]\n",
    "        all_preds.append(pred_ids_non_pad)\n",
    "        all_trues.append(label_ids_non_pad)\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_trues = torch.cat(all_trues)\n",
    "accuracy = accuracy_score(all_trues, all_preds)\n",
    "print(\"Test Accuracy:\", round(accuracy, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
